const WebCrawler = require('./src/WebCrawler');
const readline = require('readline');

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

async function main() {
  console.log('ğŸ•¸ï¸  Web Crawler - Crawl entire websites and save to JSON\n');
  
  rl.question('Enter the website URL to crawl: ', async (url) => {
    if (!url) {
      console.log('âŒ Please provide a valid URL');
      rl.close();
      return;
    }

    // Add protocol if missing
    if (!url.startsWith('http://') && !url.startsWith('https://')) {
      url = 'https://' + url;
    }

    console.log(`\nğŸš€ Starting crawl of: ${url}\n`);

    const crawler = new WebCrawler({
      maxPages: 100,        // Maximum pages to crawl
      delay: 1000,          // Delay between requests (ms)
      maxDepth: 5,          // Maximum crawl depth
      respectRobots: false, // Don't respect robots.txt - crawl freely
      timeout: 30000,       // Request timeout (ms)
      retries: 3,           // Number of retries for failed requests
      skipUnauthorized: false, // Don't skip unauthorized URLs - try to crawl everything
      ignoreRestrictions: true // Ignore all crawling restrictions
    });

    try {
      const results = await crawler.crawl(url);
      console.log(`\nâœ… Crawling completed! Found ${results.pages.length} pages`);
      console.log(`ğŸ“ Data saved to: ${results.fileName}`);
      console.log(`â±ï¸  Total time: ${results.totalTime}ms`);
      console.log(`ğŸ”— Unique links found: ${results.totalLinks}`);
      console.log(`âŒ Failed requests: ${results.failedRequests}`);
      console.log(`ğŸ”’ Skipped requests: ${results.skippedRequests}`);
    } catch (error) {
      console.error('âŒ Crawling failed:', error.message);
    }

    rl.close();
  });
}

main().catch(console.error);